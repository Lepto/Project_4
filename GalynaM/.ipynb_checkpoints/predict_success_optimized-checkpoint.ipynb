{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the coffee_shops.csv\n",
    "coffee_df = pd.read_csv(\"Resources/coffee_shops.csv\")\n",
    "coffee_df.drop(['id'], axis = 1, inplace = True)\n",
    "coffee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attr price range into separate column price\n",
    "price_df = coffee_df[['business_id','attr_value']].loc[coffee_df['attr_key']=='restaurantspricerange2']\n",
    "price_df.rename(columns = {'attr_value':'price'}, inplace = True)\n",
    "price_df.drop(price_df.loc[price_df['price'] == 'None'].index, inplace = True)\n",
    "cp_df = coffee_df.join(price_df.set_index('business_id'), on='business_id')\n",
    "cp_df.drop_duplicates(inplace = True)\n",
    "cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert attr bikeparking into separate column price\n",
    "bike_df = cp_df[['business_id','attr_value']].loc[cp_df['attr_key']=='bikeparking']\n",
    "bike_df.rename(columns = {'attr_value':'bikeparking'}, inplace = True)\n",
    "bike_df['bikeparking'].loc[bike_df['bikeparking'] == 'True'] = 1\n",
    "bike_df['bikeparking'].loc[bike_df['bikeparking'] == 'False'] = 0\n",
    "bike_df.drop(bike_df.loc[bike_df['bikeparking'] == 'None'].index, inplace = True)\n",
    "cb_df = cp_df.join(bike_df.set_index('business_id'), on='business_id')\n",
    "cb_df.drop_duplicates(inplace = True)\n",
    "cb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert attr outdoorseating into separate column outdoorseating\n",
    "out_df = cb_df[['business_id','attr_value']].loc[cb_df['attr_key']=='outdoorseating']\n",
    "out_df.rename(columns = {'attr_value':'outdoorseating'}, inplace = True)\n",
    "out_df['outdoorseating'].loc[out_df['outdoorseating'] == 'True'] = 1\n",
    "out_df['outdoorseating'].loc[out_df['outdoorseating'] == 'False'] = 0\n",
    "out_df.drop(out_df.loc[out_df['outdoorseating'] == 'None'].index, inplace = True)\n",
    "co_df = cb_df.join(out_df.set_index('business_id'), on='business_id')\n",
    "co_df.drop_duplicates(inplace = True)\n",
    "co_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove attr_key and attr_value columns as they are no longer needed\n",
    "# Clean data frame - drop duplicates, NA\n",
    "co_df.drop(['attr_key', 'attr_value'], axis = 1, inplace = True)\n",
    "co_df.drop_duplicates(inplace = True)\n",
    "co_df.dropna(inplace = True)\n",
    "co_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the generated features\n",
    "add_data_df = pd.read_csv(\"Resources/mean_features.csv\")\n",
    "add_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_df.rename(columns={\"date_diff\": \"age_in_days\", \"month_rev\": \"monthly_reviews\"}, inplace = True)\n",
    "add_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join them\n",
    "# coffee_df.set_index('business_id').join(add_data_df.set_index('b_id'))\n",
    "new_df = co_df.join(add_data_df.set_index('b_id'), on='business_id')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.price = pd.to_numeric(new_df.price, errors='coerce')\n",
    "new_df.bikeparking = pd.to_numeric(new_df.bikeparking, errors='coerce')\n",
    "new_df.outdoorseating = pd.to_numeric(new_df.outdoorseating, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "# new_df.to_csv(\"joined_data1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns\n",
    "new_df.drop(['business_id', 'name', 'address', 'city', 'state'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to numeric\n",
    "X_dummies = pd.get_dummies(new_df)\n",
    "X_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "X = X_dummies.drop(columns=[\"is_open\"])\n",
    "y = X_dummies[\"is_open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Logistic Regression\n",
    "lr1 = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {lr1.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {lr1.score(X_test_scaled, y_test)}')\n",
    "\n",
    "y_predL = lr1.predict(X_test_scaled)\n",
    "print('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_predL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AdaBoosting\n",
    "ada = AdaBoostClassifier().fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {ada.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {ada.score(X_test_scaled, y_test)}')\n",
    "\n",
    "y_predA = ada.predict(X_test_scaled)\n",
    "print('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_predA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=1).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {dt.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {dt.score(X_test_scaled, y_test)}')\n",
    "\n",
    "y_predD = dt.predict(X_test_scaled)\n",
    "print('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_predD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {rf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {rf.score(X_test_scaled, y_test)}')\n",
    "\n",
    "y_predR = rf.predict(X_test_scaled)\n",
    "print('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_predR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "import itertools\n",
    "\n",
    "classes=['Closed', 'Open']\n",
    "cm = confusion_matrix(y_test, y_predR)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# fmt = '.2f' if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predD)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# fmt = '.2f' if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = rf.feature_importances_\n",
    "# print(features)\n",
    "# plt.bar(x = range(len(features)), height=features)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.std(X_test_scaled,axis=0)*clf.feature_importances_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {}\n",
    "for feature, importance in zip(X.columns, dt.feature_importances_):\n",
    "    feats[feature] = importance\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})\n",
    "importances = importances.sort_values(by='Gini-Importance', ascending=False)\n",
    "importances = importances.reset_index()\n",
    "importances = importances.rename(columns={'index': 'Features'})\n",
    "sns.set(font_scale = 5)\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 1.7)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,15)\n",
    "sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='lightseagreen')\n",
    "plt.xlabel('Importance', fontsize=25, weight = 'bold')\n",
    "plt.ylabel('Features', fontsize=25, weight = 'bold')\n",
    "plt.title('Desicion Tree Feature Importance', fontsize=25, weight = 'bold')\n",
    "display(plt.show())\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {}\n",
    "for feature, importance in zip(X.columns, rf.feature_importances_):\n",
    "    feats[feature] = importance\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})\n",
    "importances = importances.sort_values(by='Gini-Importance', ascending=False)\n",
    "importances = importances.reset_index()\n",
    "importances = importances.rename(columns={'index': 'Features'})\n",
    "sns.set(font_scale = 5)\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 1.7)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,15)\n",
    "sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='lightseagreen')\n",
    "plt.xlabel('Importance', fontsize=25, weight = 'bold')\n",
    "plt.ylabel('Features', fontsize=25, weight = 'bold')\n",
    "plt.title('Random Forest Feature Importance', fontsize=25, weight = 'bold')\n",
    "display(plt.show())\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest did better in predicting True Positive, and features selection looks better\n",
    "### Optimize RF with hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF = gridF.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestF.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_optimized = bestF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predBR = clf_optimized.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_predBR)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "plt.title(\"Optimized Random Forest Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# fmt = '.2f' if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {}\n",
    "for feature, importance in zip(X.columns, clf_optimized.feature_importances_):\n",
    "    feats[feature] = importance\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})\n",
    "importances = importances.sort_values(by='Gini-Importance', ascending=False)\n",
    "importances = importances.reset_index()\n",
    "importances = importances.rename(columns={'index': 'Features'})\n",
    "sns.set(font_scale = 5)\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 1.7)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(30,15)\n",
    "sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='lightseagreen')\n",
    "plt.xlabel('Importance', fontsize=25, weight = 'bold')\n",
    "plt.ylabel('Features', fontsize=25, weight = 'bold')\n",
    "plt.title('Optimized Random Forest Feature Importance', fontsize=25, weight = 'bold')\n",
    "display(plt.show())\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature importances by using shap - it is using Shapley values from game theory\n",
    "# to estimate how does each feature contribute to the prediction\n",
    "explainer = shap.TreeExplainer(clf_optimized)\n",
    "shap_values = explainer.shap_values(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_values, X_test)\n",
    "# The plot below sorts features by the sum of SHAP value magnitudes over all samples,\n",
    "# and uses SHAP values to show the distribution of the impacts each feature has on the model output.\n",
    "# The color represents the feature value (red high, blue low)\n",
    "shap.summary_plot(shap_values, features=X, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting that Random Forest and Shap show different distribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features by their importance - Age\n",
    "plt.hist([new_df[new_df['is_open']==1]['age_in_days'].values,\\\n",
    "          new_df[new_df['is_open']==0]['age_in_days'].values],\\\n",
    "        label=['Open','Closed'],color=['green','red'], alpha = 0.6)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of Coffee Shops by Age')\n",
    "plt.xlabel('Coffee Shops Age based on Reviews')\n",
    "plt.ylabel('Counts of Coffee Shops')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "# plt.figure(figsize=(30,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features by their importance - Coffe Shop Stars\n",
    "plt.hist([new_df[new_df['is_open']==1]['stars'].values,\\\n",
    "          new_df[new_df['is_open']==0]['stars'].values],\\\n",
    "        label=['Open','Closed'],color=['green','red'], alpha = 0.6)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of Coffee Shops by Stars')\n",
    "plt.xlabel('Coffee Shops Stars')\n",
    "plt.ylabel('Counts of Coffee Shops')\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features by their importance - Average Number of Reviews per Month for Coffe Shops\n",
    "plt.hist([new_df[new_df['is_open']==1]['monthly_reviews'].values,\\\n",
    "          new_df[new_df['is_open']==0]['monthly_reviews'].values],\\\n",
    "        label=['Open','Closed'],color=['green','red'], alpha = 0.6)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of Coffee Shops by monthly_reviews')\n",
    "plt.xlabel('Coffee Shops average monthly reviews')\n",
    "plt.ylabel('Counts of Coffee Shops')\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (30,15)\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features by their importance - Is_chain\n",
    "plt.hist([new_df[new_df['is_open']==1]['is_chain'].values,\\\n",
    "          new_df[new_df['is_open']==0]['is_chain'].values],\\\n",
    "        label=['Open','Closed'],color=['green','red'], alpha = 0.6)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of Coffee Shops by Chain')\n",
    "plt.xlabel('Coffee Shops Is in Chain')\n",
    "plt.ylabel('Counts of Coffee Shops')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "# plt.figure(figsize=(30,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
